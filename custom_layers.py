import torch
import torch.nn as nn
import numpy as np
import torch.autograd as autograd


class NeuronFunction(autograd.Function):
    """
    class that contains Function method for the 'pool' of neurons

    Arguments:
        input(required, tensor): input signal for the network already mapped to all neurons in the 'pool'
        potentials(required, tensor): potentials of the neurons in the poll from previous step
        synapses(required, tensor): weight matrix for the neurons' connections
        tau(required, float): relaxation constant for the neurons
        g(required, float): synaptic scaling for neurons

    Output of forward step:
        r_post(tensor, (1, number of neurons)): FR of neurons
        potentials (tensor, (1, number of neurons)): potentials of neurons
    """
    @staticmethod
    def forward(ctx, input, potentials, synapses, tau, g):
        dt = 1 / tau
        r_pre = torch.tanh(potentials)
        potentials = potentials * (1 - dt) + input.sum(1).view(-1, 1) * dt + torch.matmul(g * synapses.t(), r_pre * dt)
        r_post = torch.tanh(potentials)
        return r_post, potentials

    # The backward function is not used here as it is not SGD-based method, but it is left here in case the backward
    # will be implemented further
    @staticmethod
    def backward(ctx, grad_output):
        r_pre = ctx.saved_tensors
        grad_synapses = torch.matmul(torch.matmul(P, r_pre), grad_output)
        return grad_synapses


class LinearFunction(autograd.Function):

    """class that contains Function method for the read-out layer

    Arguments:
        input_readout(required, tensor): input from the 'pool' to the read-out unit
        weight(required, tensor): weight matrix between 'pool' and read-out units

    Output of forward step:
        output(tensor, (1, number of output channels)): output signal

    """
    @staticmethod
    def forward(ctx, input_readout, weight):
        output = torch.matmul(weight.t(), input_readout)
        # ctx.save_for_backward(input)
        return output

    # The backward function is not used here as it is not SGD-based method, but it is left here in case the backward
    # will be implemented further
    @staticmethod
    def backward(ctx, grad_output):
        input = ctx.saved_tensors
        grad_weight = grad_output * torch.matmul(P, input)
        return grad_weight


class Neurons(nn.Module):
    """ custom layer for RNN 'pool'

    Arguments:
        num_neurons(required, int): number of neurons in the 'pool'
        tau(required, float): relaxation constant for the neurons
        g(required, float): synaptic scaling for neurons

    """

    def __init__(self, num_neurons, tau, g, p=1):
        super(Neurons, self).__init__()
        self.num_neurons = num_neurons
        self.tau = tau
        self.g = g
        self.p = p

        self.synapses = nn.Parameter(torch.Tensor(self.num_neurons, self.num_neurons))

    def forward(self, input, potentials):
        return NeuronFunction.apply(input, potentials, self.synapses, self.tau, self.g)


class LinearModified(nn.Module):

    """ custom layer for read out units

    Arguments:
        num_neurons(required, int): number of neurons in the 'pool'
        output_size(required, int): number of the read-out units
    """

    def __init__(self, num_neurons, output_size, p=1):
        super(LinearModified, self).__init__()
        self.num_neurons = num_neurons
        self.output_size = output_size

        # # W initialized either to zero or to values generated by a Gaussian distribution
        # # with zero mean and variance (1/(pN))
        #
        # # self.weight = nn.Parameter(torch.zeros((self.num_neurons, self.output_size)))
        #
        self.weight = nn.Parameter(torch.Tensor(self.num_neurons, self.output_size))
        # torch.nn.init.normal_(self.weight, 0, 1 / num_neurons)

    def forward(self, input_readout):
        return LinearFunction.apply(input_readout, self.weight)


